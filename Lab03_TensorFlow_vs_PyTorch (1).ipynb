{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "23ebc05e",
      "metadata": {
        "id": "23ebc05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "973aa97a-7ebf-4fc4-c75c-d2a20227e7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8634 - loss: 0.4835\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9554 - loss: 0.1509\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9685 - loss: 0.1069\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9760 - loss: 0.0808\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9794 - loss: 0.0679\n",
            "TF Training time: 32.64 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9681 - loss: 0.1041\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08877000212669373, 0.9721999764442444]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255   # Fill in normalization factor\n",
        "x_test = x_test / 255     # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # Fill input shape\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Fill number of hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Fill number of output neurons\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Fill name of loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")       # Output training time\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a",
        "outputId": "22b64076-a63c-4ca7-8d58-3ca5ee663a82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmptutc89re'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_8')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132513763736464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132513763734928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132513763734736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132520146821648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49",
        "outputId": "d48880b3-7146-4f01-8e33-595877ffa376",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 67.24 seconds\n",
            "Test accuracy: 0.9722\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time  # Added missing import\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)    # Input size 784 (28x28), output size 128\n",
        "        self.fc2 = nn.Linear(128, 10)     # Input size 128, output size 10 (digits 0-9)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))    # Apply ReLU to first layer\n",
        "        return self.fc2(x)         # Output from second layer (no activation for CrossEntropyLoss)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "WuMKMhHc8aLF",
        "outputId": "c4a564db-80e2-434c-bd57-e94b5296cde6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WuMKMhHc8aLF",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "id": "sv4P-dSS_GQB"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize pixel values to [0, 1]\n",
        "x_test = x_test / 255.0     # Normalize pixel values to [0, 1]\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Same batch size as PyTorch example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # MNIST image size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),    # 128 neurons with ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')   # 10 output classes with softmax\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "outputId": "288b24e7-6ea8-4f41-a4ab-ea82713c0646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KH-sDlHq_Gdw",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.4068, Accuracy: 0.0625\n",
            "Step 100, Loss: 0.5937, Accuracy: 0.7720\n",
            "Step 200, Loss: 0.2936, Accuracy: 0.8310\n",
            "Step 300, Loss: 0.2247, Accuracy: 0.8556\n",
            "Step 400, Loss: 0.2057, Accuracy: 0.8723\n",
            "Step 500, Loss: 0.3599, Accuracy: 0.8812\n",
            "Step 600, Loss: 0.3891, Accuracy: 0.8894\n",
            "Step 700, Loss: 0.1735, Accuracy: 0.8963\n",
            "Step 800, Loss: 0.0879, Accuracy: 0.9004\n",
            "Step 900, Loss: 0.0517, Accuracy: 0.9046\n",
            "Step 1000, Loss: 0.1653, Accuracy: 0.9079\n",
            "Step 1100, Loss: 0.2062, Accuracy: 0.9116\n",
            "Step 1200, Loss: 0.2912, Accuracy: 0.9143\n",
            "Step 1300, Loss: 0.1972, Accuracy: 0.9167\n",
            "Step 1400, Loss: 0.2843, Accuracy: 0.9189\n",
            "Step 1500, Loss: 0.0444, Accuracy: 0.9209\n",
            "Step 1600, Loss: 0.0476, Accuracy: 0.9233\n",
            "Step 1700, Loss: 0.1609, Accuracy: 0.9254\n",
            "Step 1800, Loss: 0.0397, Accuracy: 0.9270\n",
            "Training Accuracy for epoch 1: 0.9283\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1016, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0381, Accuracy: 0.9629\n",
            "Step 200, Loss: 0.1785, Accuracy: 0.9624\n",
            "Step 300, Loss: 0.2835, Accuracy: 0.9628\n",
            "Step 400, Loss: 0.1367, Accuracy: 0.9636\n",
            "Step 500, Loss: 0.0416, Accuracy: 0.9651\n",
            "Step 600, Loss: 0.3536, Accuracy: 0.9653\n",
            "Step 700, Loss: 0.2721, Accuracy: 0.9649\n",
            "Step 800, Loss: 0.0522, Accuracy: 0.9659\n",
            "Step 900, Loss: 0.0877, Accuracy: 0.9662\n",
            "Step 1000, Loss: 0.0427, Accuracy: 0.9666\n",
            "Step 1100, Loss: 0.2446, Accuracy: 0.9663\n",
            "Step 1200, Loss: 0.0427, Accuracy: 0.9665\n",
            "Step 1300, Loss: 0.1026, Accuracy: 0.9664\n",
            "Step 1400, Loss: 0.1012, Accuracy: 0.9666\n",
            "Step 1500, Loss: 0.1257, Accuracy: 0.9667\n",
            "Step 1600, Loss: 0.0942, Accuracy: 0.9673\n",
            "Step 1700, Loss: 0.2753, Accuracy: 0.9677\n",
            "Step 1800, Loss: 0.0240, Accuracy: 0.9683\n",
            "Training Accuracy for epoch 2: 0.9684\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.2593, Accuracy: 0.9062\n",
            "Step 100, Loss: 0.0607, Accuracy: 0.9765\n",
            "Step 200, Loss: 0.0191, Accuracy: 0.9771\n",
            "Step 300, Loss: 0.0975, Accuracy: 0.9754\n",
            "Step 400, Loss: 0.0557, Accuracy: 0.9751\n",
            "Step 500, Loss: 0.0267, Accuracy: 0.9752\n",
            "Step 600, Loss: 0.0583, Accuracy: 0.9748\n",
            "Step 700, Loss: 0.0415, Accuracy: 0.9755\n",
            "Step 800, Loss: 0.0181, Accuracy: 0.9757\n",
            "Step 900, Loss: 0.0948, Accuracy: 0.9759\n",
            "Step 1000, Loss: 0.3290, Accuracy: 0.9763\n",
            "Step 1100, Loss: 0.0778, Accuracy: 0.9763\n",
            "Step 1200, Loss: 0.0279, Accuracy: 0.9764\n",
            "Step 1300, Loss: 0.0717, Accuracy: 0.9764\n",
            "Step 1400, Loss: 0.0089, Accuracy: 0.9763\n",
            "Step 1500, Loss: 0.0123, Accuracy: 0.9762\n",
            "Step 1600, Loss: 0.0201, Accuracy: 0.9761\n",
            "Step 1700, Loss: 0.0193, Accuracy: 0.9762\n",
            "Step 1800, Loss: 0.0459, Accuracy: 0.9763\n",
            "Training Accuracy for epoch 3: 0.9764\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0293, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1916, Accuracy: 0.9824\n",
            "Step 200, Loss: 0.1038, Accuracy: 0.9837\n",
            "Step 300, Loss: 0.0141, Accuracy: 0.9835\n",
            "Step 400, Loss: 0.0813, Accuracy: 0.9830\n",
            "Step 500, Loss: 0.0093, Accuracy: 0.9833\n",
            "Step 600, Loss: 0.0153, Accuracy: 0.9830\n",
            "Step 700, Loss: 0.0675, Accuracy: 0.9828\n",
            "Step 800, Loss: 0.2218, Accuracy: 0.9829\n",
            "Step 900, Loss: 0.1751, Accuracy: 0.9834\n",
            "Step 1000, Loss: 0.0353, Accuracy: 0.9833\n",
            "Step 1100, Loss: 0.0202, Accuracy: 0.9833\n",
            "Step 1200, Loss: 0.0169, Accuracy: 0.9833\n",
            "Step 1300, Loss: 0.0112, Accuracy: 0.9833\n",
            "Step 1400, Loss: 0.0074, Accuracy: 0.9834\n",
            "Step 1500, Loss: 0.0134, Accuracy: 0.9832\n",
            "Step 1600, Loss: 0.0378, Accuracy: 0.9830\n",
            "Step 1700, Loss: 0.0830, Accuracy: 0.9830\n",
            "Step 1800, Loss: 0.1038, Accuracy: 0.9832\n",
            "Training Accuracy for epoch 4: 0.9834\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0400, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.1536, Accuracy: 0.9876\n",
            "Step 200, Loss: 0.0029, Accuracy: 0.9882\n",
            "Step 300, Loss: 0.0476, Accuracy: 0.9865\n",
            "Step 400, Loss: 0.0630, Accuracy: 0.9859\n",
            "Step 500, Loss: 0.0190, Accuracy: 0.9857\n",
            "Step 600, Loss: 0.0095, Accuracy: 0.9860\n",
            "Step 700, Loss: 0.0058, Accuracy: 0.9859\n",
            "Step 800, Loss: 0.0614, Accuracy: 0.9859\n",
            "Step 900, Loss: 0.0790, Accuracy: 0.9860\n",
            "Step 1000, Loss: 0.0052, Accuracy: 0.9859\n",
            "Step 1100, Loss: 0.0033, Accuracy: 0.9860\n",
            "Step 1200, Loss: 0.0556, Accuracy: 0.9861\n",
            "Step 1300, Loss: 0.0646, Accuracy: 0.9861\n",
            "Step 1400, Loss: 0.0198, Accuracy: 0.9861\n",
            "Step 1500, Loss: 0.0555, Accuracy: 0.9861\n",
            "Step 1600, Loss: 0.0049, Accuracy: 0.9860\n",
            "Step 1700, Loss: 0.0291, Accuracy: 0.9862\n",
            "Step 1800, Loss: 0.0299, Accuracy: 0.9865\n",
            "Training Accuracy for epoch 5: 0.9865\n",
            "\n",
            "TF Training time: 402.10 seconds\n",
            "Test Accuracy: 0.9742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "id": "E4Nlg4lb_qdW"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize pixel values to [0, 1]\n",
        "x_test = x_test / 255.0     # Normalize pixel values to [0, 1]\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # MNIST image size (28x28)\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),    # 128 neurons with ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')   # 10 output classes with softmax\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph for faster execution\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "outputId": "5ac43ed6-b632-4b79-bafe-23166664b237",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Jmu_hciK_qle",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3434, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.4305, Accuracy: 0.7788\n",
            "Step 200, Loss: 0.4286, Accuracy: 0.8296\n",
            "Step 300, Loss: 0.4435, Accuracy: 0.8544\n",
            "Step 400, Loss: 0.1264, Accuracy: 0.8702\n",
            "Step 500, Loss: 0.3251, Accuracy: 0.8790\n",
            "Step 600, Loss: 0.3530, Accuracy: 0.8869\n",
            "Step 700, Loss: 0.3370, Accuracy: 0.8935\n",
            "Step 800, Loss: 0.0713, Accuracy: 0.8987\n",
            "Step 900, Loss: 0.2815, Accuracy: 0.9020\n",
            "Step 1000, Loss: 0.1706, Accuracy: 0.9064\n",
            "Step 1100, Loss: 0.1594, Accuracy: 0.9091\n",
            "Step 1200, Loss: 0.2079, Accuracy: 0.9118\n",
            "Step 1300, Loss: 0.0710, Accuracy: 0.9145\n",
            "Step 1400, Loss: 0.0611, Accuracy: 0.9168\n",
            "Step 1500, Loss: 0.0600, Accuracy: 0.9189\n",
            "Step 1600, Loss: 0.2710, Accuracy: 0.9212\n",
            "Step 1700, Loss: 0.0876, Accuracy: 0.9232\n",
            "Step 1800, Loss: 0.0764, Accuracy: 0.9250\n",
            "Training Accuracy for epoch 1: 0.9262\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0425, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.1759, Accuracy: 0.9567\n",
            "Step 200, Loss: 0.0967, Accuracy: 0.9577\n",
            "Step 300, Loss: 0.0234, Accuracy: 0.9608\n",
            "Step 400, Loss: 0.0592, Accuracy: 0.9612\n",
            "Step 500, Loss: 0.1559, Accuracy: 0.9619\n",
            "Step 600, Loss: 0.3600, Accuracy: 0.9625\n",
            "Step 700, Loss: 0.1559, Accuracy: 0.9625\n",
            "Step 800, Loss: 0.1872, Accuracy: 0.9631\n",
            "Step 900, Loss: 0.0410, Accuracy: 0.9632\n",
            "Step 1000, Loss: 0.0721, Accuracy: 0.9633\n",
            "Step 1100, Loss: 0.0526, Accuracy: 0.9640\n",
            "Step 1200, Loss: 0.0404, Accuracy: 0.9637\n",
            "Step 1300, Loss: 0.0968, Accuracy: 0.9640\n",
            "Step 1400, Loss: 0.0799, Accuracy: 0.9640\n",
            "Step 1500, Loss: 0.0874, Accuracy: 0.9645\n",
            "Step 1600, Loss: 0.0749, Accuracy: 0.9652\n",
            "Step 1700, Loss: 0.0452, Accuracy: 0.9656\n",
            "Step 1800, Loss: 0.2202, Accuracy: 0.9659\n",
            "Training Accuracy for epoch 2: 0.9663\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1956, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1047, Accuracy: 0.9771\n",
            "Step 200, Loss: 0.1033, Accuracy: 0.9757\n",
            "Step 300, Loss: 0.0587, Accuracy: 0.9750\n",
            "Step 400, Loss: 0.0609, Accuracy: 0.9757\n",
            "Step 500, Loss: 0.0656, Accuracy: 0.9752\n",
            "Step 600, Loss: 0.1073, Accuracy: 0.9749\n",
            "Step 700, Loss: 0.0050, Accuracy: 0.9756\n",
            "Step 800, Loss: 0.2030, Accuracy: 0.9754\n",
            "Step 900, Loss: 0.1158, Accuracy: 0.9751\n",
            "Step 1000, Loss: 0.1790, Accuracy: 0.9752\n",
            "Step 1100, Loss: 0.0811, Accuracy: 0.9752\n",
            "Step 1200, Loss: 0.0190, Accuracy: 0.9750\n",
            "Step 1300, Loss: 0.3194, Accuracy: 0.9752\n",
            "Step 1400, Loss: 0.0506, Accuracy: 0.9752\n",
            "Step 1500, Loss: 0.0129, Accuracy: 0.9758\n",
            "Step 1600, Loss: 0.0129, Accuracy: 0.9760\n",
            "Step 1700, Loss: 0.0355, Accuracy: 0.9763\n",
            "Step 1800, Loss: 0.0520, Accuracy: 0.9762\n",
            "Training Accuracy for epoch 3: 0.9762\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0640, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0349, Accuracy: 0.9839\n",
            "Step 200, Loss: 0.0604, Accuracy: 0.9832\n",
            "Step 300, Loss: 0.0254, Accuracy: 0.9840\n",
            "Step 400, Loss: 0.0231, Accuracy: 0.9836\n",
            "Step 500, Loss: 0.1098, Accuracy: 0.9830\n",
            "Step 600, Loss: 0.0860, Accuracy: 0.9826\n",
            "Step 700, Loss: 0.0277, Accuracy: 0.9817\n",
            "Step 800, Loss: 0.0282, Accuracy: 0.9817\n",
            "Step 900, Loss: 0.0066, Accuracy: 0.9817\n",
            "Step 1000, Loss: 0.0370, Accuracy: 0.9820\n",
            "Step 1100, Loss: 0.0391, Accuracy: 0.9822\n",
            "Step 1200, Loss: 0.0079, Accuracy: 0.9824\n",
            "Step 1300, Loss: 0.1723, Accuracy: 0.9823\n",
            "Step 1400, Loss: 0.0116, Accuracy: 0.9823\n",
            "Step 1500, Loss: 0.1052, Accuracy: 0.9825\n",
            "Step 1600, Loss: 0.0969, Accuracy: 0.9823\n",
            "Step 1700, Loss: 0.0208, Accuracy: 0.9822\n",
            "Step 1800, Loss: 0.1057, Accuracy: 0.9822\n",
            "Training Accuracy for epoch 4: 0.9823\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0136, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0392, Accuracy: 0.9848\n",
            "Step 200, Loss: 0.0204, Accuracy: 0.9859\n",
            "Step 300, Loss: 0.0883, Accuracy: 0.9847\n",
            "Step 400, Loss: 0.0426, Accuracy: 0.9848\n",
            "Step 500, Loss: 0.0596, Accuracy: 0.9843\n",
            "Step 600, Loss: 0.0601, Accuracy: 0.9847\n",
            "Step 700, Loss: 0.0245, Accuracy: 0.9851\n",
            "Step 800, Loss: 0.0128, Accuracy: 0.9853\n",
            "Step 900, Loss: 0.1048, Accuracy: 0.9852\n",
            "Step 1000, Loss: 0.0131, Accuracy: 0.9854\n",
            "Step 1100, Loss: 0.0448, Accuracy: 0.9855\n",
            "Step 1200, Loss: 0.0539, Accuracy: 0.9856\n",
            "Step 1300, Loss: 0.0104, Accuracy: 0.9857\n",
            "Step 1400, Loss: 0.0543, Accuracy: 0.9858\n",
            "Step 1500, Loss: 0.0339, Accuracy: 0.9858\n",
            "Step 1600, Loss: 0.0121, Accuracy: 0.9857\n",
            "Step 1700, Loss: 0.0749, Accuracy: 0.9857\n",
            "Step 1800, Loss: 0.0350, Accuracy: 0.9862\n",
            "Training Accuracy for epoch 5: 0.9862\n",
            "\n",
            "TF Training time: 35.08 seconds\n",
            "Test Accuracy: 0.9731\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}